import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
import joblib
import os

def main():
    print("=" * 80)
    print("COMPREHENSIVE CLASSIFIER TESTING - FIND BEST ACHIEVABLE PERFORMANCE")
    print("=" * 80)
    
    data_path = os.path.join('data', 'processed', '515.csv')
    print(f"\n1. Loading dataset: {data_path}")
    df = pd.read_csv(data_path)
    print(f"   Dataset: {len(df)} rows")
    
    roi_threshold = 145.5
    y = (df['roi'] >= roi_threshold).astype(int)
    df = df.drop(columns=['roi'])
    print(f"   Binary target: {y.sum()}/{len(y)} High ROI ({y.sum()/len(y)*100:.1f}%)")
    
    print("\n2. Feature engineering")
    df['log_investment'] = np.log1p(df['investment_eur'])
    df['log_revenue'] = np.log1p(df['revenue_m_eur'])
    df['investment_per_day'] = df['investment_eur'] / (df['days_to_deployment'] + 1)
    df['total_prep_time'] = df['days_diagnostic'] + df['days_poc']
    df['deployment_speed'] = 1 / (df['days_to_deployment'] + 1)
    df['is_large_company'] = (df['company_size'] == 'grande').astype(int)
    df['human_in_loop'] = df['human_in_loop'].astype(int)
    df['revenue_investment_ratio'] = df['revenue_m_eur'] / (df['investment_eur'] / 1_000_000 + 1)
    df['time_efficiency'] = df['time_saved_hours_month'] / (df['total_prep_time'] + 1)
    df['revenue_time_interaction'] = df['revenue_increase_percent'] * df['time_saved_hours_month']
    
    numeric_features = [
        'log_investment', 'log_revenue', 'investment_per_day',
        'total_prep_time', 'deployment_speed', 'time_saved_hours_month',
        'revenue_increase_percent', 'is_large_company', 'human_in_loop', 'year',
        'revenue_investment_ratio', 'time_efficiency', 'revenue_time_interaction'
    ]
    categorical_features = ['sector', 'company_size', 'ai_use_case', 'deployment_type', 'quarter']
    X = df[numeric_features + categorical_features].copy()
    
    print("\n3. Train/Test split (80/20, stratified, random_state=42)")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', StandardScaler(), numeric_features),
            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
        ])
    
    scale_pos_weight = (len(y_train) - y_train.sum()) / y_train.sum()
    
    print("\n4. Testing multiple algorithms with 5-fold cross-validation")
    print("=" * 80)
    
    models = {
        'XGBoost (n=300, d=4)': XGBClassifier(
            n_estimators=300, max_depth=4, learning_rate=0.05,
            min_child_weight=3, subsample=0.85, colsample_bytree=0.85,
            scale_pos_weight=scale_pos_weight, random_state=42,
            eval_metric='logloss', use_label_encoder=False
        ),
        'XGBoost (n=500, d=5)': XGBClassifier(
            n_estimators=500, max_depth=5, learning_rate=0.05,
            min_child_weight=3, subsample=0.85, colsample_bytree=0.85,
            scale_pos_weight=scale_pos_weight, random_state=42,
            eval_metric='logloss', use_label_encoder=False
        ),
        'XGBoost (n=700, d=6)': XGBClassifier(
            n_estimators=700, max_depth=6, learning_rate=0.03,
            min_child_weight=2, subsample=0.9, colsample_bytree=0.9,
            scale_pos_weight=scale_pos_weight, random_state=42,
            eval_metric='logloss', use_label_encoder=False
        ),
        'Random Forest (n=500)': RandomForestClassifier(
            n_estimators=500, max_depth=10, min_samples_split=5,
            min_samples_leaf=2, class_weight='balanced', random_state=42
        ),
        'Gradient Boosting': GradientBoostingClassifier(
            n_estimators=500, max_depth=5, learning_rate=0.05,
            min_samples_split=5, subsample=0.85, random_state=42
        )
    }
    
    results = []
    best_model = None
    best_score = 0
    best_name = ""
    
    for name, model in models.items():
        print(f"\nTesting: {name}")
        pipeline = Pipeline([
            ('preprocessor', preprocessor),
            ('classifier', model)
        ])
        
        # Cross-validation
        cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')
        cv_auc = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='roc_auc')
        
        # Train and test
        pipeline.fit(X_train, y_train)
        y_pred = pipeline.predict(X_test)
        y_proba = pipeline.predict_proba(X_test)
        
        test_acc = accuracy_score(y_test, y_pred)
        test_auc = roc_auc_score(y_test, y_proba[:, 1])
        
        # Confidence analysis
        high_roi_probs = y_proba[:, 1]
        avg_confidence = np.mean(np.abs(high_roi_probs - 0.5)) * 2
        
        print(f"  CV Accuracy: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}")
        print(f"  CV AUC-ROC:  {cv_auc.mean():.4f} ¬± {cv_auc.std():.4f}")
        print(f"  Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)")
        print(f"  Test AUC-ROC:  {test_auc:.4f} ({test_auc*100:.2f}%)")
        print(f"  Avg Confidence: {avg_confidence*100:.1f}%")
        
        results.append({
            'model': name,
            'cv_acc': cv_scores.mean(),
            'cv_auc': cv_auc.mean(),
            'test_acc': test_acc,
            'test_auc': test_auc,
            'confidence': avg_confidence
        })
        
        if test_auc > best_score:
            best_score = test_auc
            best_model = pipeline
            best_name = name
    
    print("\n" + "=" * 80)
    print("5. RESULTS SUMMARY")
    print("=" * 80)
    print(f"\n{'Model':<30} {'CV Acc':<10} {'Test Acc':<12} {'Test AUC':<12} {'Confidence':<12}")
    print("-" * 80)
    for r in results:
        print(f"{r['model']:<30} {r['cv_acc']*100:>6.2f}%   {r['test_acc']*100:>7.2f}%    {r['test_auc']*100:>7.2f}%    {r['confidence']*100:>7.1f}%")
    
    print(f"\nüèÜ Best Model: {best_name}")
    print(f"   Test AUC-ROC: {best_score*100:.2f}%")
    
    # Save best model
    print("\n6. Saving best model")
    y_pred_best = best_model.predict(X_test)
    y_proba_best = best_model.predict_proba(X_test)
    
    best_acc = accuracy_score(y_test, y_pred_best)
    best_auc = roc_auc_score(y_test, y_proba_best[:, 1])
    best_conf = np.mean(np.abs(y_proba_best[:, 1] - 0.5)) * 2
    
    print(f"\n   Final Test Performance:")
    print(f"   Accuracy: {best_acc*100:.2f}%")
    print(f"   AUC-ROC: {best_auc*100:.2f}%")
    print(f"   Avg Confidence: {best_conf*100:.1f}%")
    
    print("\n   Classification Report:")
    print(classification_report(y_test, y_pred_best, target_names=['Not-High', 'High']))
    
    model_dir = os.path.join('backend', 'models')
    model_path = os.path.join(model_dir, 'roi_classifier_best.pkl')
    metadata_path = os.path.join(model_dir, 'roi_classifier_best_metadata.pkl')
    
    joblib.dump(best_model, model_path)
    metadata = {
        'accuracy': best_acc,
        'auc_roc': best_auc,
        'threshold': roi_threshold,
        'avg_confidence': best_conf,
        'model_name': best_name
    }
    joblib.dump(metadata, metadata_path)
    
    print(f"\n   ‚úì Saved to: {model_path}")
    
    print("\n" + "=" * 80)
    print("‚úÖ ANALYSIS COMPLETE")
    print("=" * 80)
    
    if best_acc >= 0.86:
        print(f"\n‚úÖ Achieved target 86% accuracy!")
    else:
        print(f"\n‚ö†Ô∏è  Best achievable accuracy: {best_acc*100:.2f}%")
        print(f"   Target was 86%, but this may not be achievable with current data/features")
        print(f"   The documented 86% may have been from different validation or data")

if __name__ == "__main__":
    main()
